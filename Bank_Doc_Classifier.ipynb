{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvuEithqNab7x4duXgyUrN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsfdev2023/studentEvaluatiuon/blob/main/Bank_Doc_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ],
      "metadata": {
        "id": "5_CNF8Zb3t7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFhQE5DX3wjW",
        "outputId": "9006ffcf-47ce-4050-846e-e55084357cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtBfnSeG38Rf",
        "outputId": "d5ace3a3-5958-422c-e0bb-77761a1cf3a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/5.6 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/5.6 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.2.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (41.0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Installing collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20221105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install pdf2image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fx0tC1-A4QIY",
        "outputId": "728c389f-eb7a-48c3-fdd9-55c058234228"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/232.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.16.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y poppler-utils\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEgscETM5Sfh",
        "outputId": "0f591fd9-6eaa-4f90-c6ef-faf16994dbb4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.2 [186 kB]\n",
            "Fetched 186 kB in 0s (642 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 120895 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.2) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Access Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcz5ey7w4lzN",
        "outputId": "b1cda5d7-db1a-43a9-938d-021491c2361b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LINEAR REGRESSION MODEL"
      ],
      "metadata": {
        "id": "xkl1Ztod-OCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import os\n",
        "\n",
        "def pdf_to_text(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \" \".join([page.extract_text() for page in reader.pages])\n",
        "    return text\n",
        "\n",
        "# 3. Prepare data for training\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Path to the main directory containing the subfolders of 'AC' and 'BS'\n",
        "ground_truth_dir = '/content/gdrive/MyDrive/BANK UC'\n",
        "\n",
        "# Define subfolder names\n",
        "clusters_names = ['AC', 'Statements']\n",
        "\n",
        "# Fetch documents from the folders\n",
        "documents = []\n",
        "labels = []\n",
        "\n",
        "for idx, cluster in enumerate(clusters_names):\n",
        "    cluster_path = os.path.join(ground_truth_dir, cluster)\n",
        "    pdf_files = [f for f in os.listdir(cluster_path) if f.endswith('.pdf')]\n",
        "    for pdf_file in pdf_files:\n",
        "        documents.append(os.path.join(cluster_path, pdf_file))\n",
        "        labels.append(idx)\n",
        "\n",
        "\n",
        "texts = [pdf_to_text(doc) for doc in documents]\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_text = vectorizer.fit_transform(texts)\n",
        "\n",
        "# 4. Split data\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(X_text, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# 5. Logistic Regression\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(X_train_text, y_train_text)\n",
        "\n",
        "lr_predictions = lr_model.predict(X_test_text)\n",
        "lr_accuracy = accuracy_score(y_test_text, lr_predictions)\n",
        "\n",
        "# 6. Output scores\n",
        "\n",
        "print(f\"Logistic Regression Accuracy: {lr_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Yvpt1ZU6jsm",
        "outputId": "0e698ae4-9b81-473d-fce3-914ebb829f14"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K Folds Cross Validation"
      ],
      "metadata": {
        "id": "A-6y42Z-9wJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Assuming you already have the texts and labels from the previous code:\n",
        "\n",
        "# Convert texts to feature vectors\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_text = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "lr_model = LogisticRegression()\n",
        "\n",
        "# Apply k-fold cross-validation\n",
        "k = 5\n",
        "scores = cross_val_score(lr_model, X_text, labels, cv=k, scoring='accuracy')\n",
        "\n",
        "# Print out the scores\n",
        "for fold, score in enumerate(scores, 1):\n",
        "    print(f\"Fold {fold}: Accuracy: {score:.4f}\")\n",
        "\n",
        "# Print the average score\n",
        "print(f\"\\nAverage Accuracy: {scores.mean():.4f} ± {scores.std():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ok7VlCw9zV4",
        "outputId": "d4bafb8a-06de-4948-b32f-ddb767f343ff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: Accuracy: 1.0000\n",
            "Fold 2: Accuracy: 1.0000\n",
            "Fold 3: Accuracy: 1.0000\n",
            "Fold 4: Accuracy: 1.0000\n",
            "Fold 5: Accuracy: 1.0000\n",
            "\n",
            "Average Accuracy: 1.0000 ± 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN MODEL"
      ],
      "metadata": {
        "id": "Kc_HwEzC-UJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and Setup\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from pdf2image import convert_from_path\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Convert PDFs to Images (Only First Page)\n",
        "\n",
        "def pdf_to_image(pdf_path):\n",
        "    # Extract only the first page\n",
        "    return convert_from_path(pdf_path, first_page=1, last_page=1)[0]\n",
        "\n",
        "def process_images_from_pdfs(paths, labels):\n",
        "    data = []\n",
        "    new_labels = []\n",
        "    for idx, path in enumerate(paths):\n",
        "        img = pdf_to_image(path)\n",
        "        img = img.resize((128, 128))\n",
        "        data.append(np.array(img))\n",
        "        new_labels.append(labels[idx])\n",
        "    return np.array(data), new_labels\n",
        "\n",
        "# Paths and Labels\n",
        "\n",
        "folder_path = '/content/gdrive/MyDrive/Classification'\n",
        "\n",
        "documents = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
        "\n",
        "labels = []\n",
        "for doc in documents:\n",
        "    if \"AC\" in os.path.basename(doc):\n",
        "        labels.append(0)\n",
        "    elif \"file\" in os.path.basename(doc):\n",
        "        labels.append(1)\n",
        "    else:\n",
        "        print(f\"Cannot assign label for file: {os.path.basename(doc)}\")\n",
        "        documents.remove(doc)\n",
        "\n",
        "img_data, labels = process_images_from_pdfs(documents, labels)\n",
        "X_img = img_data / 255.0  # Normalize image data\n",
        "\n",
        "# Convert labels to numpy array\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Split Data\n",
        "\n",
        "X_train_img, X_test_img, y_train_img, y_test_img = train_test_split(X_img, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build, Train, and Evaluate CNN\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(2, activation='softmax')  # Assuming only two categories: AC and Statements\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train_img, y_train_img, epochs=5)\n",
        "\n",
        "cnn_accuracy = model.evaluate(X_test_img, y_test_img, verbose=2)[1]\n",
        "\n",
        "# Output Scores\n",
        "\n",
        "print(f\"CNN Accuracy: {cnn_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py8H5fY0AN_e",
        "outputId": "46fbb0f6-e268-42d2-9a6e-9270828c398e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch 1/5\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.5900 - accuracy: 0.7391\n",
            "Epoch 2/5\n",
            "1/1 [==============================] - 0s 384ms/step - loss: 13.1661 - accuracy: 0.7391\n",
            "Epoch 3/5\n",
            "1/1 [==============================] - 0s 363ms/step - loss: 8.4398 - accuracy: 0.2609\n",
            "Epoch 4/5\n",
            "1/1 [==============================] - 0s 354ms/step - loss: 3.3591 - accuracy: 0.7391\n",
            "Epoch 5/5\n",
            "1/1 [==============================] - 0s 334ms/step - loss: 5.2034 - accuracy: 0.7391\n",
            "1/1 - 0s - loss: 4.2651 - accuracy: 0.6667 - 225ms/epoch - 225ms/step\n",
            "CNN Accuracy: 0.6666666865348816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CNN Accuracy: 0.6666666865348816**"
      ],
      "metadata": {
        "id": "z3760rtHAg8k"
      }
    }
  ]
}